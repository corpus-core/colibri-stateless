"""
Integration tests for Colibri Python bindings

These tests use file-based mocking to test the complete RPC pipeline 
against real test data from the test/data directory.
"""

import pytest
from pathlib import Path
from typing import List, Dict, Any

from colibri.testing import (
    discover_tests, 
    run_test_case,
    FileBasedMockStorage,
    FileBasedMockRequestHandler
)
from colibri import Colibri


class TestIntegration:
    """Integration tests using file-based test data"""
    
    def setup_method(self):
        """Setup for each test method"""
        self.test_data_root = Path(__file__).parent / '..' / '..' / '..' / 'test' / 'data'
        self.test_data_root = self.test_data_root.resolve()
        
    def test_discover_tests(self):
        """Test that test discovery finds test cases"""
        test_cases = discover_tests(self.test_data_root)
        
        # Should find at least some test cases
        assert len(test_cases) > 0, "Should discover at least one test case"
        
        # Check structure of discovered tests
        for test_case in test_cases[:5]:  # Check first 5
            assert 'name' in test_case
            assert 'directory' in test_case
            assert 'method' in test_case
            assert 'params' in test_case
            assert 'chain_id' in test_case
            
            # Verify directory exists
            assert test_case['directory'].exists()
            assert test_case['directory'].is_dir()
            
            # Verify test.json exists
            test_json_path = test_case['directory'] / 'test.json'
            assert test_json_path.exists()
            
        print(f"‚úÖ Discovered {len(test_cases)} test cases successfully")
        
    @pytest.mark.asyncio
    async def test_file_based_mock_storage(self):
        """Test FileBasedMockStorage functionality"""
        
        # Find a test case with known storage files
        test_cases = discover_tests(self.test_data_root)
        if not test_cases:
            pytest.skip("No test cases found")
            
        # Use the first test case
        test_case = test_cases[0]
        test_dir = test_case['directory']
        
        storage = FileBasedMockStorage(test_dir)
        
        # List files in test directory
        storage_files = [f for f in test_dir.iterdir() 
                        if f.is_file() and f.name not in ['test.json']]
        
        if storage_files:
            # Test getting existing file
            storage_file = storage_files[0]
            result = storage.get(storage_file.name)
            assert result is not None
            assert isinstance(result, bytes)
            assert len(result) > 0
            
            # Verify content matches file
            expected_content = storage_file.read_bytes()
            assert result == expected_content
            
        # Test getting non-existent file
        result = storage.get('non_existent_file')
        assert result is None
        
        print(f"‚úÖ FileBasedMockStorage working correctly")
        
    @pytest.mark.asyncio
    async def test_file_based_mock_request_handler(self):
        """Test FileBasedMockRequestHandler functionality"""
        
        # Find a test case
        test_cases = discover_tests(self.test_data_root)
        if not test_cases:
            pytest.skip("No test cases found")
            
        test_case = test_cases[0]
        test_dir = test_case['directory']
        
        handler = FileBasedMockRequestHandler(test_dir)
        
        # Create a mock request (beacon API example)
        from colibri.types import DataRequest
        
        request = DataRequest(
            req_ptr=12345,
            url='eth/v1/beacon/headers/head',
            method='GET',
            encoding='json',
            request_type='beacon_api'
        )
        
        try:
            response = await handler.handle_request(request)
            assert isinstance(response, bytes)
            assert len(response) > 0
            print(f"‚úÖ FileBasedMockRequestHandler: Got response ({len(response)} bytes)")
        except Exception as e:
            # Expected if no matching file exists
            print(f"‚ÑπÔ∏è FileBasedMockRequestHandler: No mock file found (expected): {e}")


# =============================================================================
# PARAMETERIZED INTEGRATION TESTS
# =============================================================================

def pytest_generate_tests(metafunc):
    """
    Pytest hook to dynamically generate test parameters.
    
    This creates one test per discovered test case from test/data directories.
    """
    if "integration_test_case" in metafunc.fixturenames:
        # Discover all test cases
        test_data_root = Path(__file__).parent / '..' / '..' / '..' / 'test' / 'data'
        test_data_root = test_data_root.resolve()
        
        test_cases = discover_tests(test_data_root)
        
        if not test_cases:
            # If no test cases found, create a dummy one to avoid empty test suite
            test_cases = [{'name': 'no_tests_found', 'skip': True}]
        
        # Create test IDs for better test output
        test_ids = [test_case['name'] for test_case in test_cases]
        
        metafunc.parametrize("integration_test_case", test_cases, ids=test_ids)


class TestIntegrationCases:
    """Dynamic integration test cases generated from test/data"""
    
    @pytest.mark.asyncio
    async def test_integration_case(self, integration_test_case: Dict[str, Any]):
        """
        Run a single integration test case
        
        This test is automatically parametrized by pytest_generate_tests
        to run once for each discovered test case.
        """
        
        # Skip dummy test case
        if integration_test_case.get('skip'):
            pytest.skip("No integration test cases found in test/data")
        
        # Run the test case
        result = await run_test_case(integration_test_case)
        
        # Assert based on test result
        if result['status'] == 'PASSED':
            # Test passed
            assert True
            print(f"‚úÖ Integration test {result['name']} passed")
            
        elif result['status'] == 'FAILED':
            # Test failed - result mismatch
            assert False, f"Test {result['name']} failed: {result['error']}"
            
        elif result['status'] == 'ERROR':
            # Test had an error
            # For now, we'll mark these as expected failures since some tests
            # might require specific chain states or network conditions
            pytest.xfail(f"Test {result['name']} had error: {result['error']}")
        
        else:
            assert False, f"Unknown test status: {result['status']}"


# =============================================================================
# MANUAL INTEGRATION TESTS FOR SPECIFIC CASES
# =============================================================================

class TestSpecificIntegrationCases:
    """Specific integration tests for important cases"""
    
    @pytest.mark.asyncio
    async def test_eth_blockNumber_integration(self):
        """Test eth_blockNumber with real test data"""
        
        test_data_root = Path(__file__).parent / '..' / '..' / '..' / 'test' / 'data'
        test_data_root = test_data_root.resolve()
        
        # Look for eth_blockNumber test case
        blocknum_dir = test_data_root / 'eth_blockNumber_electra'
        if not blocknum_dir.exists():
            pytest.skip("eth_blockNumber_electra test case not found")
        
        # Load test configuration
        test_json_path = blocknum_dir / 'test.json'
        if not test_json_path.exists():
            pytest.skip("test.json not found in eth_blockNumber_electra")
            
        import json
        with open(test_json_path) as f:
            config = json.load(f)
        
        # Create mocks
        mock_storage = FileBasedMockStorage(blocknum_dir)
        mock_request_handler = FileBasedMockRequestHandler(blocknum_dir)
        
        # Create client
        client = Colibri(
            chain_id=config['chain_id'],
            storage=mock_storage,
            request_handler=mock_request_handler
        )
        
        # Test method support
        method_type = client.get_method_support(config['method'])
        print(f"üîç Method {config['method']} support: {method_type}")
        
        # Execute RPC call
        result = await client.rpc(config['method'], config['params'])
        
        # Verify result format
        assert isinstance(result, str)
        assert result.startswith('0x')
        
        # Compare with expected if available
        if 'expected_result' in config:
            assert result == config['expected_result'], \
                f"Expected {config['expected_result']}, got {result}"
        
        print(f"‚úÖ eth_blockNumber integration test passed: {result}")
        
    @pytest.mark.asyncio
    async def test_eth_getBalance_integration(self):
        """Test eth_getBalance with real test data"""
        
        test_data_root = Path(__file__).parent / '..' / '..' / '..' / 'test' / 'data'
        test_data_root = test_data_root.resolve()
        
        # Look for eth_getBalance test case
        balance_dirs = [
            test_data_root / 'eth_getBalance_electra',
            test_data_root / 'eth_getBalance1'
        ]
        
        balance_dir = None
        for dir_path in balance_dirs:
            if dir_path.exists():
                balance_dir = dir_path
                break
                
        if not balance_dir:
            pytest.skip("No eth_getBalance test case found")
        
        # Load test configuration
        test_json_path = balance_dir / 'test.json'
        if not test_json_path.exists():
            pytest.skip(f"test.json not found in {balance_dir.name}")
            
        import json
        with open(test_json_path) as f:
            config = json.load(f)
        
        # Create mocks
        mock_storage = FileBasedMockStorage(balance_dir)
        mock_request_handler = FileBasedMockRequestHandler(balance_dir)
        
        # Create client
        client = Colibri(
            chain_id=config['chain_id'],
            storage=mock_storage,
            request_handler=mock_request_handler
        )
        
        # Execute RPC call
        result = await client.rpc(config['method'], config['params'])
        
        # Verify result format
        assert isinstance(result, str)
        assert result.startswith('0x')
        
        # Compare with expected if available
        if 'expected_result' in config:
            assert result == config['expected_result'], \
                f"Expected {config['expected_result']}, got {result}"
        
        print(f"‚úÖ eth_getBalance integration test passed: {result}")